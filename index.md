# Data Description
Measurements of line pressure and fluid temperature before and after the block are taken, the temperature of the copper block, and eleven different streamwise temperature profiles between the copper block and cold plate in order to calculate the convective heat transfer coefficient along the cold plate from the sCO2. Data is taken at 12 different chiller temperatures to perform a temperature sweep to ensure supercriticality is reached. There are three different datasets involved in my research. Dynamic and static data are generally collected in a similar way, both using sensors like thermocouples, coliolis meter, pressure transducers, and readings given by operating devices. These sensors are plugged into a DAC card and read through LabVIEW to be later collected and placed into an excel spreadsheet. Mapped data differs from dynamic and static data in that it is collected using a mapped array of single temperatures spaced along a wire, which is then routed through the cold plate to generate a map of temperatures in a spreadsheet. The software attached to this sensor is called ODiSI and generates a temperature profile in the streamwise direction of CO2 flow. In total, this experiment is expected to be about 20 gb. Below are the categories for datatypes, along with their approximated sizes: <br />

<br /> Dynamic Data: (2000 kb)
* inlet/outlet temperature 
* inlet/outlet pressure
* Mass flow rate
* Density
* Block Temperature

<br /> Static Data: (120 kb)
* Pump speed 
* Chiller mass flow rate
* Chiller inlet temperature

<br /> Mapped Data: (57.6 mb)
* LUNA fiber temperature sensor

<br /> Metadata: (12 kb)
* Matlab code used for data analysis 
* LabVIEW block diagrams with setup 

# Roles and responsibilities 
The data management roles for the project defined in DMP 1 are primarily handled by myself and another researcher working on a similar project on the same test setup. Roles that can be split between the projects (e.g. those that involve both projects) are split between myself and the other graduate researcher, such as instrumentation maintenance, quality control, experimental setup construction, DMP implementation, access control, and software creation. The rest of the roles are fulfilled by the individual researcher as it pertains to their own project. Roles such as archiving, data manager, protection of sensitive data, data organization, metadata generation, and data analysis are handled on a per-project basis. Another role outside of the ones discussed in the previous class specific to this project includes safety checks. As both I and the other researcher are investigating supercritical fluids at high pressures and temperatures, it’s important that both of us maintain constant checks on our work to ensure that the quality of our work and the equipment being used will hold up to the safety standards of the lab we operate in. 

# Data standards and metadata
The design of the data collection process and documentation for my current research project follows something similar to the process described by the IEEE NetCDF paper on data management. There are some key differences however in the exact implementation of the design process of both organization and communication. For starters, the data collected during the course of this research project is similar to the multidimensional approach of NetCDF. There is a three-dimensional array that describes the x, y surface temperature distribution over a surface over a third dimension of time. This method of data management can be easily expanded to include other variables of interest, such as heat transfer coefficients and density distributions. <br /> 

The metadata associated with this project mainly concerns how the data was recorded and what measurements were recorded. The majority of the metadata in the project is defining variables and their significance, with units and processes defined in both the code and in the dataset. This includes descriptions of setting up the LUNA fiber system used to record axial temperature distribution, how to record the information from the given software, and the devices used to record other data. The code also provides information about how the data was recorded and what device was used to record it to make reproduction easier for future research.  This metadata is made during the course of research, primarily before data collection begins. Variables and processes are labeled and defined before data collection, while the three-dimensional array is filled in after collection has been completed.<br /> 

To organize the files in a directory for future use, the approach used for this project is similar to Noble outlined in PLOS Computation Biology in which files are given a top-level organization tree to sort them by what they represent to the research. Data, documents, results, and codes are stored as separate branches in which each one is given a name based on what it is and when it was created or downloaded. Since most of the research in this project is done though Matlab and Python, it would be inefficient to re-date each of these versions as they update. For this, GitHub is to be used as both a collaborative measure between researchers and a way to date file changes. Using GitHub, researchers involved in the creation of data tools are able to change elements of a code without creating a separate file in the file directory. For those not able to access the GitHub page, comments on the code are included to guide users on how the code works and what variables represent. <br /> 

https://data.mendeley.com/datasets/7hm73vk33r/1 Submitted for the ‘Find a Dataset’ assignment, this gives a good description of the type of metadata tracking I intend to use during the course of my research. Separate spreadsheets are included for each of the trails for a change in parameter. The variables are clearly defined on the first spreadsheet with a brief description before the other sheets detail the results from each trail. This is similar to the method planned for this project. Data will be separated into separate three-dimensional arrays with variables defined clearly for the viewer. I’m unsure of the method in which I plan to present the data in a viewable format across platforms, however it could be possible to give array samples of the start, middle, and end of the transient data to give a complete picture of development. <br />

# Storage and security 
The data I am collecting is being used by a private company in the development of a cooling system for a grant project, and therefore falls under OSU’s definition of sensitive data. This means that data collected for this project must be protected to a reasonable degree and is not be exposed to the public. Data for this project is stored and collected on lab computers, while collective data about the line setup and components between the line are stored on a shared box with the other graduate researcher using the same test setup. The lab computer is locked behind a password and is therefore only accessible by the primary research and myself. <br />
During the project, data related to the results of the project are stored locally on a machine located in the locked laboratory where the project is being worked. This ensures that the data has no chance of being disseminated before its intended release. This, of course, lends itself to being easily wiped if the hard drive used to store the data decides to randomly corrupt. To prevent this, an external flash drive is used to store the data as a separate digital copy apart from the one stored locally, as well as storing the data in a cloud drive only shared to the primary investigator and researcher. This process requires uploading each file manually, and therefore has the potential to loose files due to forgetting to place an extra copy in the drive. Fortunatly, cloud storage allows for instant uploads of data, meaning that if data is not moved to the external drive it will be located on the cloud as well. 

# Access and Data Sharing / Archiving and Preservation 
Sharing data is important to advance research in a collaborative effort between researchers. There are, however, important caveats that require data be stored securely or strictly internally such as NDA agreements on data or other confidential aspects of research for private companies/institutions. The project that I am currently working on requires it be kept internal to the project sponsor, which means data collected over the course of the project will be kept private until a limited release is approved. This limited release can include conclusions and methods, along with the metadata affiliated with them. <br /> 
To ensure this data stays preserved for future use, two copies are created on physical devices/drives. All data and processes used to collect the data are stored locally on a computer that can only be accessed by myself, as well as a separate physical drive that is archived with other previous work done in the lab. Because work as a graduate student is temporary, access to data is also shared with the PI of the project. Beyond the principle investigator, the company funding the research conducted through this lab will also archive the data and will take the responsibility of storing the data for longer if needed. These archived projects are kept for a period of 5 years before they are reorganized to make space for new projects. Organization inside the project files then is key to ensure that reproducibility is possible in the future. A document is included with instructions on how to reproduce the experiment, as well as detailed metadata associated with how each variable is included in the project files. The space where the storage drives are held is locked until access is given by the PI. The files that are preserved in project files can vary due to the different software that is employed from project to project. For this research, there are six main project file types: 
1.	Matlab code 
2.	LabVIEW case files
3.	Excel spreadsheets
4.	PDF documents 
5.	JPEG files
6.	LunaFiber case downloads <br />

<br /> Many of these files are accessed only through proprietary software that requires money to access. Matlab, LabVIEW, and LunaFiber ODISI program files require software subscriptions and unfortunately cannot be changed to any other format. This, of course, can be remedied by including descriptions of their purpose and an instructional guide as to how they can be created on other software, however in the long run this damages the project’s ability to be shared between platforms. 
